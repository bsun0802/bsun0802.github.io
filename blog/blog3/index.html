<!DOCTYPE html>

<html lang="en-us"><head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css"
        integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Nanum+Myeongjo&family=Noto+Serif+JP&family=Cormorant+Garamond&family=Libre+Baskerville&family=Source+Serif+Pro&family=Crimson+Text&family=Inter&family=Crimson+Pro&family=Literata&family=Ubuntu+Mono&family=Inter&family=Roboto">
    <link rel="stylesheet" type="text/css" href="/css/style.css">

    
    

    <title>Bo Sun | Facebook AI | How to correctly apply Batch Normalization layer in TensorFlow（the pitfall of UPDATE_OPS)</title>


    

</head><body class="container d-flex flex-column min-vh-100">

<div class="blog_nav_bar secondary_font ">
    
    
    <a class="navbar-brand" href="/">about</a>
    
    
    
    <a class="navbar-brand" href="/blog">« all posts</a>
    
    
</div>



<h3>
    <a class="title" href="/blog/blog3/">How to correctly apply Batch Normalization layer in TensorFlow（the pitfall of UPDATE_OPS)</a>
</h3>

<div class="reading_time secondary_font text-muted ">
    <span>
        May 12 2018 · 2 min read
    </span>

</div>




<div class="tags_navigation">
    
    <a class="tag" href="/tags/deep-learning/">#deep-learning</a>
    
    <a class="tag" href="/tags/ai/">#ai</a>
    
    <a class="tag" href="/tags/tensor-flow/">#tensor-flow</a>
    
</div>


<p><strong>Caution: this post is written in 2018 and maybe out-of-date!</strong></p>
<div class="toc ">
    <div>
        <strong>Table of Contents</strong>
    </div>
    <div>
        
        <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#batch-normalization-layer-in-tensorflow">Batch Normalization layer in TensorFlow</a></li>
        <li><a href="#how-to-apply-batch-normalization-in-tensorflow">How to apply batch-normalization in TensorFlow</a></li>
        <li><a href="#the-pitfall-update_ops">The pitfall: UPDATE_OPS</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
</div>
<h2 id="batch-normalization-layer-in-tensorflow">Batch Normalization layer in TensorFlow</h2>
<p>In this post, I&rsquo;m not going to explain the math behind what batch normalization does and why it could help if we apply batch-normalization in our deep networks.</p>
<p>For a quick look of what batch normalization is, check this <a href="https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad">post</a>.</p>
<p>For a deeper understanding of batch normalization, you can look up in the paper &ldquo;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&rdquo; by <a href="https://arxiv.org/abs/1502.03167">Ioeffe and Szegedy (2015)</a>.</p>
<h2 id="how-to-apply-batch-normalization-in-tensorflow">How to apply batch-normalization in TensorFlow</h2>
<p>Thanks to the tensor flow API, we could apply batch-normalization as easy as we implement any other layers(eg. relu, dropout, etc). We simply call <a href="https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"><code>tf.layers.batch_normalization</code></a> to create a batch-normalization layer. And as it is clearly noted that: make sure that <code>training</code> parameter is correctly set. We could declare a boolean placeholder to make sure that during training <code>training</code> is set to <code>True</code>, and when evaluating <code>training</code> is set to <code>False</code>. Because when inference(validation), we do not calculate the statistics(moving mean and moving variance) from validation data, instead we applied those calculated in the current training batch.</p>
<h2 id="the-pitfall-update_ops">The pitfall: UPDATE_OPS</h2>
<p>You might be confused that, after applying the batch-normalization layer, your network performs properly in training, maybe even achieve higher training accuracy, but the validation accuracy drops significantly for no reason.</p>
<p>This is because in TensorFlow, the moving mean and moving variance need to be updated manually. It is clearly stated in a blue box in the documentation here<a href="https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"><code>tf.layers.batch_normalization</code></a>. But it is common for beginners to overlook those because the ease and homogeneity of applying other layers in TensorFlow.</p>
<p>So when define <code>train_op</code> if you used batch-normalization, you need to create the dependency of <code>UPDATE_OPS</code> on your <code>train_op</code> like following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">  x_norm <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>batch_normalization(x, training<span style="color:#f92672">=</span>training)
  
  <span style="color:#75715e"># ...</span>

  update_ops <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>get_collection(tf<span style="color:#f92672">.</span>GraphKeys<span style="color:#f92672">.</span>UPDATE_OPS)
  <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>control_dependencies(update_ops):
    train_op <span style="color:#f92672">=</span> optimizer<span style="color:#f92672">.</span>minimize(loss)
</code></pre></div>

<footer class="mt-auto d-flex justify-content-center text-muted small secondary_font">
    <span class="text-muted">Copyright (c) 2021, Bo Sun,
        <a class="text-muted" href="https://github.com/hadisinaee/avicenna" target="_blank"> created by Avicenna
            (MIT)</a>
    </span>
</footer><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx"
    crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/feather-icons/4.28.0/feather.min.js"></script>
<script>
    feather.replace()
</script></body>

</html>